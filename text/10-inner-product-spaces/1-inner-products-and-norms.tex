\section{Inner Products and Norms}
\label{sec:10_1}

The dot product was introduced in $\RR^n$ to provide a natural generalization of the geometrical notions of length and orthogonality that were so important in Chapter~\ref{chap:4}. The plan in this chapter is to define an \textit{inner product} on an arbitrary real vector space $V$ (of which the dot product is an example in $\RR^n$) and use it to introduce these concepts in $V$. While this causes some repetition of arguments in Chapter \ref{chap:8}, it is well worth the effort because of the much wider scope of the results when stated in full generality. 

\begin{definition}{Inner Product Spaces}{030288}
An \textbf{inner product}\index{inner product!defined} on a real vector space $V$ is a function that assigns a real number $\langle\vect{v}, \vect{w}\rangle$ to every pair $\vect{v}$, $\vect{w}$ of vectors in $V$ in such a way that the following axioms are satisfied.
\end{definition}
\begin{itemize}
\item[\textit{P1.}]  $\langle\vect{v}, \vect{w}\rangle$ \textit{is a real number for all} $\vect{v}$ \textit{and} $\vect{w}$ \textit{in} $V$.

\item[\textit{P2.}]  $\langle\vect{v}, \vect{w}\rangle = \langle\vect{w}, \vect{v}\rangle$ \textit{for all} $\vect{v}$ \textit{and} $\vect{w}$ \textit{in} $V$.

\item[\textit{P3.}]  $\langle\vect{v} + \vect{w}, \vect{u}\rangle = \langle\vect{v}, \vect{u}\rangle + \langle\vect{w}, \vect{u}\rangle$ \textit{for all} $\vect{u}$, $\vect{v}$, \textit{and} $\vect{w}$ \textit{in} $V$.

\item[\textit{P4.}] $\langle r\vect{v}, \vect{w}\rangle = r\langle\vect{v}, \vect{w}\rangle$ \textit{for all} $\vect{v}$ \textit{and} $\vect{w}$ \textit{in} $V$ \textit{and all} $r$ \textit{in} $\RR$.

\item[\textit{P5.}]  $\langle\vect{v}, \vect{v}\rangle > 0$ \textit{for all} $\vect{v} \neq \vect{0}$ \textit{in} $V$.

\end{itemize}

\noindent A real vector space $V$ with an inner product $\langle\ , \rangle$ will be called an \textbf{inner product space}\index{inner product space!defined}.  Note that every subspace of an inner product space is again an inner product space using the same inner product.\footnote{If we regard $\mathbb{C}^n$ as a vector space over the field $\mathbb{C}$  of complex numbers, then the ``standard inner product'' on $\mathbb{C}^n$ defined in Section~\ref{sec:8_6} does not satisfy Axiom P4 (see Theorem~\ref{thm:025575}(3)).}

\begin{example}{}{030303}
$\RR^n$ is an inner product space with the dot product as inner product\index{dot product!as inner product}\index{set of all ordered $n$-tuples ($\RR^n$)!as inner product space}:
\begin{equation*}
\langle \vect{v}, \vect{w} \rangle = \vect{v} \dotprod \vect{w} \quad \mbox{ for all } \vect{v},  \vect{w} \in \RR^n
\end{equation*}
See Theorem~\ref{thm:014833}. This is also called the
\textbf{euclidean}\index{euclidean inner product}\index{inner product!euclidean inner product} inner product, and
$\RR^n$, equipped with the dot product, is called \textbf{euclidean}
$n$-\textbf{space}\index{euclidean $n$-space}. 
\end{example}

\begin{example}{}{030310}
If $A$ and $B$ are $m \times n$ matrices, define $\langle A, B\rangle = \func{tr}(AB^{T})$ where $\func{tr}(X)$ is the trace of the square matrix $X$. Show that $\langle\ , \rangle$ is an inner product in $\vectspace{M}_{mn}$.

\begin{solution}
P1 is clear. Since $\func{tr}(P) = \func{tr}(P^{T})$ for every square matrix $P$, we have P2:
\begin{equation*}
\langle A, B \rangle = \func{tr}(AB^T) = \func{tr}[(AB^T)^T] = \func{tr}(BA^T) = \langle B, A \rangle 
\end{equation*}
Next, P3 and P4 follow because trace is a linear transformation $\vectspace{M}_{mn} \to \RR$ (Exercise \ref{ex:10_1_19}). Turning to P5, let $\vect{r}_{1}, \vect{r}_{2}, \dots, \vect{r}_{m}$ denote the rows of the matrix $A$. Then the $(i, j)$-entry of $AA^{T}$ is $\vect{r}_{i} \dotprod \vect{r}_{j}$, so
\begin{equation*}
\langle A, A \rangle = \func{tr}(AA^T) = 
\vect{r}_1 \dotprod \vect{r}_1 + 
\vect{r}_2 \dotprod \vect{r}_2 + \dots +
\vect{r}_m \dotprod \vect{r}_m
\end{equation*}
But $\vect{r}_{j} \dotprod \vect{r}_{j}$ is the sum of the squares of
the entries of $\vect{r}_{j}$, so this shows that $\langle A, A\rangle$ is the sum of the squares of all $nm$ entries of $A$. Axiom P5 follows.
\end{solution}
\end{example}

The importance of the next example in analysis is difficult to overstate. 

\begin{example}{\footnotemark}{030334}
Let $\vectspace{C}[a, b]$ denote the vector space of
\textbf{continuous functions}\index{continuous functions}\index{function!continuous functions}\index{vector spaces!continuous functions} from $[a, b]$ to $\RR$, a subspace of $\vectspace{F}[a, b]$. Show that
\begin{equation*}
\langle f, g \rangle = \int_{a}^{b} f(x)g(x)dx
\end{equation*}
defines an inner product on $\vectspace{C}[a, b]$.

\begin{solution}
Axioms P1 and P2 are clear. As to axiom P4,
\begin{equation*}
\langle rf, g \rangle = \int_{a}^{b} rf(x)g(x)dx = r\int_{a}^{b} f(x)g(x)dx = 
r\langle f, g \rangle
\end{equation*}
Axiom P3 is similar. Finally, theorems of calculus show that $\langle f, f \rangle = \int_{a}^{b} f(x)^2dx \geq 0$ and, if $f$ is continuous, that this is zero if and only if $f$ is the zero function. This gives axiom P5.
\end{solution}
\end{example}
\footnotetext{This example (and others later that refer to it) can be omitted with no loss of continuity by students with no calculus background.}

If $\vect{v}$ is any vector, then, using axiom P3, we get
\begin{equation*}
\langle \vect{0}, \vect{v} \rangle = \langle \vect{0} + \vect{0}, \vect{v} \rangle =
\langle \vect{0}, \vect{v} \rangle + \langle \vect{0}, \vect{v} \rangle
\end{equation*}
and it follows that the number $\langle\vect{0}, \vect{v}\rangle$ must be zero. This observation is recorded for reference in the following theorem, along with several other properties of inner products. The other proofs are left as Exercise \ref{ex:10_1_20}.

\begin{theorem}{}{030346}
Let $\langle\ , \rangle$ be an inner product on a space $V$; let $\vect{v}$, $\vect{u}$, and $\vect{w}$ denote vectors in $V$; and let $r$ denote a real number.

\begin{enumerate}
\item $\langle \vect{u}, \vect{v} + \vect{w}\rangle =
  \langle\vect{u}, \vect{v}\rangle + \langle\vect{u}, \vect{w}\rangle$

\item $\langle\vect{v}, r\vect{w}\rangle =  r\langle\vect{v}, \vect{w}\rangle = \langle r\vect{v}, \vect{w}\rangle$

\item $\langle\vect{v}, \vect{0}\rangle = 0 = \langle\vect{0}, \vect{v} \rangle $

\item $\langle\vect{v}, \vect{v}\rangle = 0$ if and only if $\vect{v} = \vect{0}$

\end{enumerate}\index{inner product!properties of}
\end{theorem}

If $\langle\ , \rangle$ is an inner product on a space $V$, then, given $\vect{u}$, $\vect{v}$, and $\vect{w}$ in $V$,
\begin{equation*}
\langle r\vect{u} + s\vect{v}, \vect{w} \rangle = \langle r\vect{u}, \vect{w} \rangle + \langle s\vect{v}, \vect{w} \rangle = r\langle \vect{u}, \vect{w} \rangle + s\langle \vect{v}, \vect{w} \rangle
\end{equation*}
for all $r$ and $s$ in $\RR$ by axioms P3 and P4. Moreover, there is nothing special about the fact that there are two terms in the linear combination or that it is in the first component:
\begin{eqnarray*}
\langle r_1\vect{v}_1 + r_2\vect{v}_2 + \dots + r_n\vect{v}_n, \vect{w} \rangle &=& 
r_1\langle \vect{v}_1, \vect{w} \rangle + 
r_2\langle \vect{v}_2, \vect{w} \rangle + \dots +
r_n\langle \vect{v}_n, \vect{w} \rangle \mbox{, and }\\
\langle \vect{v}, s_1\vect{w}_1 + s_2\vect{w}_2 + \dots + s_m\vect{w}_m \rangle &=&
s_1\langle \vect{v}, \vect{w}_1 \rangle +
s_2\langle \vect{v}, \vect{w}_2 \rangle + \dots +
s_m\langle \vect{v}, \vect{w}_m \rangle
\end{eqnarray*}
hold for all $r_{i}$ and $s_{i}$ in $\RR$ and all $\vect{v}$, $\vect{w}$, $\vect{v}_{i}$, and $\vect{w}_{j}$ in $V$. These results are described by saying that inner products ``preserve'' linear combinations. For example,
\begin{align*}
\langle 2\vect{u} - \vect{v}, 3\vect{u} + 2\vect{v} \rangle &= 
\langle 2\vect{u}, 3\vect{u} \rangle + \langle 2\vect{u}, 2\vect{v} \rangle + \langle -\vect{v}, 3\vect{u} \rangle + \langle -\vect{v}, 2\vect{v} \rangle \\
&= 6 \langle \vect{u}, \vect{u} \rangle + 4 \langle \vect{u}, \vect{v} \rangle -3 \langle \vect{v}, \vect{u} \rangle - 2 \langle \vect{v}, \vect{v} \rangle \\
&= 6 \langle \vect{u}, \vect{u} \rangle + \langle \vect{u}, \vect{v} \rangle - 2 \langle \vect{v}, \vect{v} \rangle 
\end{align*}

If $A$ is a symmetric $n \times n$ matrix and $\vect{x}$ and $\vect{y}$ are columns in $\RR^n$, we regard the $1 \times 1$ matrix $\vect{x}^{T}A\vect{y}$ as a number. If we write
\begin{equation*}
\langle \vect{x}, \vect{y} \rangle = \vect{x}^TA\vect{y} \quad \mbox{ for all columns } \vect{x}, \vect{y} \mbox{ in } \RR^n
\end{equation*}
then axioms P1--P4 follow from matrix arithmetic (only P2 requires that $A$ is symmetric). Axiom P5 reads
\begin{equation*}
\vect{x}^TA \vect{x} > 0 \quad \mbox{ for all columns } \vect{x} \neq \vect{0} \mbox{ in } \RR^n
\end{equation*}
and this condition characterizes the positive definite matrices\index{positive definite matrix}\index{square matrix ($n \times n$ matrix)!positive definite matrix} (Theorem~\ref{thm:024830}). This proves the first assertion in the next theorem.

\begin{theorem}{}{030372}
If $A$ is any $n \times n$ positive definite matrix, then
\begin{equation*}
\langle \vect{x}, \vect{y} \rangle = \vect{x}^TA\vect{y} \mbox{ for all columns } \vect{x}, \vect{y} \mbox{ in } \RR^n
\end{equation*}
defines an inner product on $\RR^n$, and every inner product on $\RR^n$ arises in this way.\index{inner product!positive definite $n \times n$ matrix}
\end{theorem}

\begin{proof}
Given an inner product $\langle\ , \rangle$ on $\RR^n$, let $\{\vect{e}_{1}, \vect{e}_{2}, \dots, \vect{e}_{n}\}$ be the standard basis of $\RR^n$. If $\vect{x} = \displaystyle \sum_{i = 1}^{n} x_i\vect{e}_i$ and $\vect{y} = \displaystyle \sum_{j = 1}^{n} y_j\vect{e}_j$ are two vectors in $\RR^n$, compute $\langle\vect{x}, \vect{y}\rangle$ by adding the inner product of each term $x_{i}\vect{e}_{i}$ to each term $y_{j}\vect{e}_{j}$. The result is a double sum.
\begin{equation*}
\langle \vect{x}, \vect{y} \rangle = \displaystyle \sum_{i = 1}^{n} \sum_{j = 1}^{n} \langle x_i \vect{e}_i, y_j\vect{e}_j \rangle =
\displaystyle \sum_{i = 1}^{n} \sum_{j = 1}^{n} x_i \langle \vect{e}_i, \vect{e}_j \rangle y_j
\end{equation*}
As the reader can verify, this is a matrix product:
\begin{equation*}
\langle \vect{x}, \vect{y} \rangle =
\leftB \begin{array}{cccc}
x_1 & x_2 & \cdots & x_n \\
\end{array} \rightB
\leftB \begin{array}{cccc}
\langle \vect{e}_1, \vect{e}_1 \rangle & \langle \vect{e}_1, \vect{e}_2 \rangle & \cdots & \langle \vect{e}_1, \vect{e}_n \rangle \\
\langle \vect{e}_2, \vect{e}_1 \rangle & \langle \vect{e}_2, \vect{e}_2 \rangle & \cdots & \langle \vect{e}_2, \vect{e}_n \rangle \\
\vdots & \vdots & \ddots & \vdots \\
\langle \vect{e}_n, \vect{e}_1 \rangle & \langle \vect{e}_n, \vect{e}_2 \rangle & \cdots & \langle \vect{e}_n, \vect{e}_n \rangle \\
\end{array} \rightB
\leftB \begin{array}{c}
	y_1 \\
	y_2 \\
	\vdots \\
	y_n
\end{array} \rightB
\end{equation*}
Hence $\langle\vect{x}, \vect{y}\rangle = \vect{x}^{T}A\vect{y}$, where $A$ is the $n \times n$ matrix whose $(i, j)$-entry is $\langle\vect{e}_{i}, \vect{e}_{j} \rangle$. The fact that 
\begin{equation*}
\langle\vect{e}_{i}, \vect{e}_{j}\rangle = \langle\vect{e}_{j}, \vect{e}_{i}\rangle\end{equation*}
shows that $A$ is symmetric. Finally, $A$ is positive definite by Theorem~\ref{thm:024830}.
\end{proof}

\noindent Thus, just as every linear operator $\RR^n \to \RR^n$ corresponds to an $n \times n$ matrix, every inner product on $\RR^n$ corresponds to a positive definite $n \times n$ matrix. In particular, the dot product corresponds to the identity matrix $I_{n}$.

\vspace{1em}
\noindent{\sl\textbf{Remark}}

\noindent If we refer to the inner product space $\RR^n$ without specifying the inner product, we mean that the dot product is to be used.\index{dot product!inner product space}\index{inner product space!dot product!use of}

\bigskip

\begin{example}{}{030413}
Let the inner product $\langle\ , \rangle$ be defined on $\RR^2$ by
\begin{equation*}
\left \langle 
\leftB \begin{array}{c}
v_1 \\
v_2
\end{array} \rightB, \leftB \begin{array}{c}
w_1 \\
w_2
\end{array} \rightB
\right \rangle
= 2v_1w_1 - v_1w_2 - v_2w_1 + v_2w_2
\end{equation*}
Find a symmetric $2 \times 2$ matrix $A$ such that $\langle\vect{x}, \vect{y}\rangle = \vect{x}^{T}A\vect{y}$ for all $\vect{x}$, $\vect{y}$ in $\RR^2$.

\begin{solution}
The $(i, j)$-entry of the matrix $A$ is the coefficient of $v_{i}w_{j}$ in the expression, so 
$ A =
\leftB \begin{array}{rr}
2 & -1 \\
-1 & 1
\end{array} \rightB$. Incidentally, if 
$\vect{x} = 
\leftB \begin{array}{r}
x \\
y
\end{array} \rightB$, then
\begin{equation*}
\langle \vect{x}, \vect{x} \rangle = 2x^2 - 2xy + y^2 = x^2 +(x - y)^2 \geq 0
\end{equation*}

for all $\vect{x}$, so $\langle\vect{x}, \vect{x}\rangle = 0$ implies
$\vect{x} = \vect{0}$. Hence $\langle\ , \rangle$ is indeed an inner product, so $A$ is positive definite.
\end{solution}
\end{example}

Let $\langle\ , \rangle$ be an inner product on $\RR^n$ given as in Theorem~\ref{thm:030372} by a positive definite matrix $A$. If $\vect{x} = 
\leftB \begin{array}{cccc}
x_1 & x_2 & \cdots & x_n
\end{array} \rightB^T $, then $\langle\vect{x}, \vect{x}\rangle = \vect{x}^{T}A\vect{x}$ is an expression in the variables
$x_{1}, x_{2}, \dots, x_{n}$ called a \textbf{quadratic form}\index{quadratic form}. These are studied in detail in Section~\ref{sec:8_8}.

\subsection*{Norm and Distance}

\begin{definition}{Norm and Distance}{030438}
As in $\RR^n$, if $\langle\ , \rangle$ is an inner product on a space $V$, the \textbf{norm}\footnotemark \index{norm}\index{inner product!and norms}\index{inner product space!norms}
 $\vectlength\vect{v}\vectlength$ of a vector $\vect{v}$ in $V$ is defined by
\begin{equation*}
\vectlength \vect{v} \vectlength = \sqrt{\langle \vect{v}, \vect{v} \rangle}
\end{equation*}
We define the \textbf{distance}\index{distance}\index{inner product space!distance} between vectors $\vect{v}$ and $\vect{w}$ in an inner product space $V$ to be
\begin{equation*}
\func{d}(\vect{v}, \vect{w}) = \vectlength \vect{v} - \vect{w} \vectlength
\end{equation*}
\end{definition}
\footnotetext{If the dot product is used in $\RR^n$, the norm $\vectlength\vect{x}\vectlength$ of a vector $\vect{x}$ is usually called the \textbf{length}\index{vectors!length}\index{dot product!length}\index{length!norm, where dot product used} of $\vect{x}$.}

\noindent Note that axiom P5 guarantees that
$\langle\vect{v}, \vect{v}\rangle \geq 0$, so $\vectlength\vect{v}\vectlength$ is a real number.

\begin{example}{}{030446}
\begin{wrapfigure}{l}{6cm} 
\vspace*{-1.5em}
\centering
\input{10-inner-product-spaces/figures/1-inner-products-and-norms/example10.1.5}
%\captionof{figure}{\label{fig:030449}}
\end{wrapfigure}

\setlength{\rightskip}{0pt plus 200pt}
The norm of a continuous function $f = f(x)$ in $\vectspace{C}[a, b]$ (with the inner product from Example~\ref{exa:030334}) is given by
\begin{equation*}
\vectlength f \vectlength = \sqrt{\int_{a}^{b} f(x)^2dx}
\end{equation*}
Hence $\vectlength f\vectlength^{2}$ is the area beneath the graph of $y = f(x)^{2}$ between $x = a$ and $x = b$ (shaded in the diagram).
\end{example}

\begin{example}{}{030454}
Show that $\langle\vect{u} + \vect{v}, \vect{u} - \vect{v}\rangle = \vectlength\vect{u}\vectlength^{2} - \vectlength\vect{v}\vectlength^{2}$ in any inner product space.

\begin{solution}
\vspace*{-2em}\begin{align*}
\langle \vect{u} + \vect{v}, \vect{u} - \vect{v} \rangle &= \langle \vect{u}, \vect{u} \rangle - \langle \vect{u}, \vect{v} \rangle + \langle \vect{v}, \vect{u} \rangle - \langle \vect{v}, \vect{v} \rangle \\
&= \vectlength \vect{u} \vectlength^2 - \langle \vect{u}, \vect{v} \rangle + \langle \vect{u}, \vect{v} \rangle - \vectlength \vect{v} \vectlength^2 \\
&= \vectlength \vect{u} \vectlength^2 - \vectlength \vect{v} \vectlength^2
\end{align*}
\end{solution}
\end{example}

A vector $\vect{v}$ in an inner product space $V$ is called a \textbf{unit vector}\index{unit vector}\index{inner product space!unit vector} if $\vectlength\vect{v}\vectlength = 1$. The set of all unit vectors in $V$ is called the \textbf{unit ball}\index{unit ball} in $V$. For example, if $V = \RR^2$ (with the dot product) and $\vect{v} = (x, y)$, then
\begin{equation*}
\vectlength \vect{v} \vectlength^2 = 1 \quad \mbox{ if and only if } \quad x^2 + y^2 = 1
\end{equation*}
Hence the unit ball in $\RR^2$ is the \textbf{unit circle}\index{unit circle} $x^{2} + y^{2} = 1$ with centre at the origin and radius $1$. However, the shape of the unit ball varies with the choice of inner product.

\begin{example}{}{030469}
\begin{wrapfigure}[11]{l}{5cm} 
\centering
\input{10-inner-product-spaces/figures/1-inner-products-and-norms/example10.1.7}
%\captionof{figure}{\label{fig:030476}}
\end{wrapfigure}

\setlength{\rightskip}{0pt plus 200pt}
Let $a > 0$ and $b > 0$. If $\vect{v} = (x, y)$ and $\vect{w} = (x_{1}, y_{1})$, define an inner product on $\RR^2$ by
\begin{equation*}
\langle \vect{v}, \vect{w} \rangle = \frac{xx_1}{a^2} + \frac{yy_1}{b^2}
\end{equation*}
The reader can verify (Exercise \ref{ex:10_1_5}) that this is indeed an inner product. In this case
\begin{equation*}
\vectlength \vect{v} \vectlength^2 = 1 \quad \mbox{ if and only if } \quad \frac{x^2}{a^2} + \frac{y^2}{b^2} = 1
\end{equation*}
so the unit ball is the ellipse shown in the diagram.
\end{example}

\noindent Example~\ref{exa:030469} graphically illustrates the fact that norms and distances in an inner product space $V$ vary with the choice of inner product in $V$.

\begin{theorem}{}{030480}
If $\vect{v} \neq \vect{0}$ is any vector in an inner product space $V$, then $\frac{1}{\vectlength \vect{v} \vectlength} \vect{v}$ is the unique unit vector that is a positive multiple of $\vect{v}$.
\end{theorem}

The next theorem reveals an important and useful fact about the relationship between norms and inner products, extending the Cauchy inequality for $\RR^n$ (Theorem~\ref{thm:014907}).

\begin{theorem}{Cauchy-Schwarz Inequality\footnotemark}{030486}
If $\vect{v}$ and $\vect{w}$ are two vectors in an inner product space $V$, then
\begin{equation*}
\langle \vect{v}, \vect{w} \rangle^2 \leq \vectlength \vect{v} \vectlength^2 \vectlength \vect{w} \vectlength^2
\end{equation*}
Moreover, equality occurs if and only if one of $\vect{v}$ and $\vect{w}$ is a scalar multiple of the other.
\end{theorem}
\footnotetext{Hermann Amandus Schwarz (1843--1921) was a German mathematician at the University of Berlin. He had strong geometric intuition, which he applied with great ingenuity to particular problems. A version of the inequality appeared in 1885.}

\begin{proof}
Write $\vectlength\vect{v}\vectlength = a$ and $\vectlength\vect{w}\vectlength = b$. Using Theorem~\ref{thm:030346} we compute:
\begin{equation}
\label{eq:thm10_1_4}
\begin{split}
	\vectlength b\vect{v} - a \vect{w} \vectlength^2 &= b^2 \vectlength \vect{v} \vectlength^2 - 2ab \langle \vect{v}, \vect{w} \rangle + a^2\vectlength \vect{w} \vectlength^2 = 2ab(ab - \langle \vect{v}, \vect{w} \rangle) \\
	\vectlength b\vect{v} + a \vect{w} \vectlength^2 &= b^2 \vectlength \vect{v} \vectlength^2 + 2ab \langle \vect{v}, \vect{w} \rangle + a^2\vectlength \vect{w} \vectlength^2 = 2ab(ab + \langle \vect{v}, \vect{w} \rangle) \\
\end{split}
\end{equation}
It follows that $ab - \langle\vect{v}, \vect{w}\rangle \geq 0$ and
$ab + \langle\vect{v}, \vect{w}\rangle \geq 0$, and hence that $-ab \leq \langle\vect{v}, \vect{w}\rangle \leq ab$. But then $| \langle\vect{v}, \vect{w}\rangle | \leq ab = \vectlength\vect{v}\vectlength \vectlength \vect{w}\vectlength$, as desired.

Conversely, if $|\langle \vect{v},  \vect{w}\rangle | =
\vectlength\vect{v}\vectlength \vectlength \vect{w} \vectlength = ab$
then $\langle\vect{v}, \vect{w}\rangle = \pm ab$. Hence (\ref{eq:thm10_1_4}) shows that $b\vect{v} - a\vect{w} = \vect{0}$ or $b\vect{v} + a\vect{w} = \vect{0}$. It follows that one of $\vect{v}$ and $\vect{w}$ is a scalar multiple of the other, even if $a = 0$ or $b = 0$.
\end{proof}

\begin{example}{}{030499}
If $f$ and $g$ are continuous functions on the interval $[a, b]$, then (see Example~\ref{exa:030334})
\begin{equation*}
\left(\int_{a}^{b} f(x)g(x)dx \right) ^2 \leq \int_{a}^{b} f(x)^2 dx \int_{a}^{b} g(x)^2 dx
\end{equation*}
\end{example}

Another famous inequality, the so-called \textit{triangle inequality}, also comes from the Cauchy-Schwarz inequality. It is included in the following list of basic properties of the norm of a vector.

\begin{theorem}{}{030504}
If $V$ is an inner product space, the norm $\vectlength \dotprod \vectlength$ has the following properties.

\begin{enumerate}
\item $\vectlength\vect{v}\vectlength \geq 0$ for every vector $\vect{v}$ in $V$.

\item $\vectlength\vect{v}\vectlength = 0$ if and only if $\vect{v} = \vect{0}$.

\item $\vectlength r \vect{v}\vectlength = |r|\vectlength\vect{v}\vectlength$ for every $\vect{v}$ in $V$ and every $r$ in $\RR$.

\item $\vectlength\vect{v} + \vect{w}\vectlength \leq \vectlength\vect{v}\vectlength + \vectlength\vect{w}\vectlength$ for all $\vect{v}$ and $\vect{w}$ in $V$ (\textbf{triangle inequality}\index{triangle inequality}\index{absolute value!triangle inequality}).

\end{enumerate}
\end{theorem}

\begin{proof}
Because $\vectlength \vect{v} \vectlength = \sqrt{\langle \vect{v}, \vect{v} \rangle}$, properties (1) and (2) follow immediately from (3) and (4) of Theorem~\ref{thm:030346}. As to (3), compute
\begin{equation*}
\vectlength r\vect{v} \vectlength ^2 = \langle r\vect{v}, r\vect{v} \rangle = r^2\langle \vect{v}, \vect{v} \rangle = r^2\vectlength \vect{v} \vectlength^2
\end{equation*}
Hence (3) follows by taking positive square roots. Finally, the fact that $\langle\vect{v}, \vect{w}\rangle \leq \vectlength\vect{v}\vectlength\vectlength\vect{w}\vectlength$ by the Cauchy-Schwarz inequality gives
\begin{align*}
\vectlength \vect{v} + \vect{w} \vectlength ^2 = 
\langle \vect{v} + \vect{w}, \vect{v} + \vect{w} \rangle &=
\vectlength \vect{v} \vectlength ^2 + 2 \langle \vect{v}, \vect{w} \rangle +
\vectlength \vect{w} \vectlength ^2 \\
&\leq \vectlength \vect{v} \vectlength ^2 + 
2 \vectlength \vect{v} \vectlength \vectlength \vect{w} \vectlength + 
\vectlength \vect{w} \vectlength ^2 \\
&= (\vectlength \vect{v} \vectlength + \vectlength \vect{w} \vectlength)^2
\end{align*}
Hence (4) follows by taking positive square roots.
\end{proof}

\noindent It is worth noting that the usual triangle inequality for absolute values,
\begin{equation*}
| r + s | \leq |r| + |s| \mbox{ for all real numbers } r \mbox{ and } s
\end{equation*}
is a special case of (4) where $V = \RR = \RR^1$ and the dot product $\langle r, s \rangle = rs$ is used.

In many calculations in an inner product space, it is required to show that some vector $\vect{v}$ is zero. This is often accomplished most easily by showing that its norm $\vectlength\vect{v}\vectlength$ is zero. Here is an example.

\begin{example}{}{030528}
Let $\{\vect{v}_{1}, \dots, \vect{v}_{n}\}$ be a spanning set for an inner product space $V$. If $\vect{v}$ in $V$ satisfies $\langle\vect{v}, \vect{v}_{i}\rangle = 0$ for each $i = 1, 2, \dots, n$, show that $\vect{v} = \vect{0}$.

\begin{solution}
Write $\vect{v} = r_{1}\vect{v}_{1} + \dots + r_{n}\vect{v}_{n}$, $r_{i}$ in $\RR$. To show that $\vect{v} = \vect{0}$, we show that $\vectlength\vect{v}\vectlength^{2} = \langle\vect{v}, \vect{v}\rangle = 0$. Compute:
\begin{equation*}
\langle \vect{v}, \vect{v} \rangle 
= \langle \vect{v}, r_1\vect{v}_1 + \dots + r_n\vect{v}_n \rangle 
= r_1\langle \vect{v}, \vect{v}_1 \rangle + \dots + r_n \langle \vect{v}, \vect{v}_n \rangle
= 0
\end{equation*}
by hypothesis, and the result follows.
\end{solution}
\end{example}

The norm properties in Theorem~\ref{thm:030504} translate to the following properties of distance familiar from geometry. The proof is Exercise \ref{ex:10_1_21}.

\begin{theorem}{}{030545}
Let $V$ be an inner product space.

\begin{enumerate}
\item $\func{d}(\vect{v}, \vect{w}) \geq 0$ for all $\vect{v}$, $\vect{w}$ in $V$.

\item $\func{d}(\vect{v}, \vect{w}) = 0$ if and only if $\vect{v} = \vect{w}$.

\item $\func{d}(\vect{v}, \vect{w}) = \func{d}(\vect{w}, \vect{v})$ for all $\vect{v}$ and $\vect{w}$ in $V$.

\item $\func{d}(\vect{v}, \vect{w}) \leq \func{d}(\vect{v}, \vect{u}) + \func{d}(\vect{u}, \vect{w})$ for all $\vect{v}$, $\vect{u}$, and $\vect{w}$ in $V$.

\end{enumerate}
\end{theorem}

\section*{Exercises for \ref{sec:10_1}}

\begin{Filesave}{solutions}
\solsection{Section~\ref{sec:10_1}}
\end{Filesave}

\begin{multicols}{2}
\begin{ex}
In each case, determine which of axioms P1--P5 fail to hold.

\begin{enumerate}[label={\alph*.}]
\item $V = \RR^2$, $\langle (x_1, y_1), (x_2, y_2) \rangle = x_1y_1x_2y_2$

\item $V = \RR^3$, \\$\langle (x_1, x_2, x_3), (y_1, y_2, y_3) \rangle = x_1y_1 - x_2y_2 + x_3y_3$

\item $V = \mathbb{C}$, $\langle z, w \rangle = z\overline{w}$, where $\overline{w}$ is complex
conjugation

\item $V = \vectspace{P}_3$, $\langle p(x), q(x) \rangle = p(1)q(1)$

\item $V = \vectspace{M}_{22}$, $\langle A, B \rangle = \func{det}(AB)$

\item $V = \vectspace{F}[0, 1]$, $\langle f, g \rangle = f(1)g(0) + f(0)g(1)$

\end{enumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  P5 fails.

\setcounter{enumi}{3}
\item  P5 fails.

\setcounter{enumi}{5}
\item  P5 fails.

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
Let $V$ be an inner product space. If $U \subseteq V$ is a subspace, show that $U$ is an inner product space using the same inner product.

\begin{sol}
Axioms P1--P5 hold in $U$ because they hold in $V$.
\end{sol}
\end{ex}

\begin{ex}
In each case, find a scalar multiple of $\vect{v}$ that is a unit vector.

\begin{enumerate}[label={\alph*.}]
\item $\vect{v} = f$ in $\vectspace{C}[0, 1]$ where 
$f(x) = x^2$  \\ $\langle f, g \rangle \int_{0}^{1} f(x)g(x)dx$

\item $\vect{v} = f$ in $\vectspace{C}[-\pi, \pi]$ where 
$f(x) = \cos x$ \\ $\langle f, g \rangle \int_{-\pi}^{\pi} f(x)g(x)dx$

\item $\vect{v} = 
\leftB \begin{array}{r}
1 \\
3
\end{array} \rightB$
in $\RR^2$ where $\langle \vect{v}, \vect{w} \rangle = \vect{v}^T
\leftB \begin{array}{rr}
1 & 1 \\
1 & 2
\end{array} \rightB
\vect{w}$

\item $ \vect{v} = 
\leftB \begin{array}{r}
3 \\
-1
\end{array} \rightB$
in $\RR^2$, $\langle \vect{v}, \vect{w} \rangle = \vect{v}^T
\leftB \begin{array}{rr}
1 & -1 \\
-1 & 2
\end{array} \rightB
\vect{w}$

\end{enumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  $\frac{1}{\sqrt{\pi}}f$

\setcounter{enumi}{3}
\item  
$\frac{1}{\sqrt{17}}
\leftB \begin{array}{r}
3 \\
-1
\end{array} \rightB$

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
In each case, find the distance between $\vect{u}$ and $\vect{v}$.

\begin{enumerate}[label={\alph*.}]
\item $\vect{u} = (3, -1, 2, 0)$, $\vect{v} = (1, 1, 1, 3); 
\langle \vect{u}, \vect{v} \rangle = \vect{u} \dotprod \vect{v}$

\item $\vect{u} = (1,  2, -1, 2)$, $\vect{v} = (2, 1, -1, 3); 
\langle \vect{u}, \vect{v} \rangle = \vect{u} \dotprod \vect{v}$

\item $\vect{u} = f$, $\vect{v} = g $ in $\vectspace{C}[0, 1]$ where $f(x) = x^2 $ and $g(x) = 1 - x$; $\langle f, g \rangle = \int_{0}^{1} f(x)g(x)dx$

\item $\vect{u} = f$, $\vect{v} = g $ in $\vectspace{C}[-\pi, \pi]$ where $f(x) = 1$ and $g(x) = \cos x$; $\langle f, g \rangle = \int_{-\pi}^{\pi} f(x)g(x)dx$
\end{enumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  $\sqrt{3}$

\setcounter{enumi}{3}
\item  $\sqrt{3\pi}$

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex} \label{ex:10_1_5}
Let $a_{1}, a_{2}, \dots, a_{n}$ be positive numbers. Given $\vect{v} = (v_{1}, v_{2}, \dots, v_{n})$ and $\vect{w} = (w_{1}, w_{2}, \dots, w_{n})$, define $\langle\vect{v}, \vect{w}\rangle = a_{1}v_{1}w_{1} + \dots + a_{n}v_{n}w_{n}$. Show that this is an inner product on $\RR^n$.
\end{ex}

\begin{ex}
If $\{\vect{b}_{1}, \dots, \vect{b}_{n}\}$ is a basis of $V$ and if
$\vect{v} = v_1\vect{b}_1 + \dots + v_n\vect{b}_n$ and 
$\vect{w} = w_1\vect{b}_1 + \dots + w_n\vect{b}_n$ are vectors in $V$, define
\begin{equation*}
\langle \vect{v}, \vect{w} \rangle = v_1w_1 + \dots + v_nw_n .
\end{equation*}
Show that this is an inner product on $V$.
\end{ex}

\begin{ex}
If $p = p(x)$ and $q = q(x)$ are polynomials in $\vectspace{P}_{n}$, define
\begin{equation*}
\langle p, q \rangle = p(0)q(0) + p(1)q(1) + \dots + p(n)q(n)
\end{equation*}
Show that this is an inner product on $\vectspace{P}_{n}$. \newline [\textit{Hint for P5}: Theorem~\ref{thm:020203} or Appendix~\ref{chap:appdpolynomials}.]
\end{ex}

\begin{ex}
Let $\vectspace{D}_{n}$ denote the space of all functions from the set
$\{1, 2, 3, \dots, n\}$ to $\RR$ with pointwise addition and
scalar multiplication (see Exercise~\ref{ex:ex6_3_35}). Show
that $\langle\ , \rangle$ is an inner product on $\vectspace{D}_{n}$ if \newline $\langle\vect{f}, \vect{g}\rangle = f(1)g(1) + f(2)g(2) + \dots + f(n)g(n)$.

\begin{sol}
P1 and P2 are clear since $f(i)$ and $g(i)$ are real numbers.

\vspace*{-1.5em}\begin{flalign*}
\mbox{P3: } \langle f + g, h \rangle &= \sum_{i}(f + g)(i) \dotprod h(i) &\\
&= \sum_{i}(f(i) + g(i)) \dotprod h(i) &\\
&= \sum_{i}[f(i)h(i) + g(i)h(i)] &\\
&= \sum_{i}f(i)h(i) + \sum_{i}g(i)h(i) &\\
&= \langle f, h \rangle + \langle g, h \rangle. &\\
\mbox{P4: } \hspace{1em}\langle rf, g \rangle &= \sum_{i}(rf)(i) \dotprod g(i) &\\
&= \sum_{i}rf(i) \dotprod g(i) &\\
&= r \sum_{i}f(i) \dotprod g(i) &\\
&= r\langle f, g \rangle &\\
\end{flalign*}

\vspace*{-2em}\noindent P5: If $ f \neq 0 $, then $\langle f, f \rangle = \displaystyle \sum_{i}f(i)^2 > 0 $ because some $f(i) \neq 0$.
\end{sol}
\end{ex}

\begin{ex}
Let $\func{re}(z)$ denote the real part of the complex number
$z$. Show that $\langle\ , \rangle$ is an inner product on $\mathbb{C}$ if $\langle\vect{z}, \vect{w}\rangle = \func{re}(z\overline{w})$.
\end{ex}

\begin{ex}
If $T : V \to V$ is an isomorphism of the inner product space $V$, show that
\begin{equation*}
\langle \vect{v}, \vect{w} \rangle_1 = \langle T(\vect{v}), T(\vect{w}) \rangle
\end{equation*}
defines a new inner product $\langle\ , \rangle_{1}$ on $V$.
\end{ex}

\begin{ex}
Show that every inner product $\langle\ , \rangle$ on $\RR^n$ has the form $\langle\vect{x}, \vect{y}\rangle = (U\vect{x}) \dotprod (U\vect{y})$ for some upper triangular matrix $U$ with positive diagonal entries. [\textit{Hint}: Theorem~\ref{thm:024907}.]
\end{ex}

\begin{ex}
In each case, show that $\langle\vect{v}, \vect{w}\rangle = \vect{v}^{T}A\vect{w}$ defines an inner product on $\RR^2$ and hence show that $A$ is positive definite.
\begin{exenumerate}
\exitem 
$A = 
\leftB \begin{array}{rr}
2 & 1 \\
1 & 1
\end{array} \rightB$
\exitem 
$A =
\leftB \begin{array}{rr}
5 & -3 \\
-3 & 2
\end{array} \rightB$
\exitem
$A =
\leftB \begin{array}{rr}
3 & 2 \\
2 & 3
\end{array} \rightB$
\exitem 
$A = 
\leftB \begin{array}{rr}
3 & 4 \\
4 & 6
\end{array} \rightB$
\end{exenumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  $  \langle \vect{v}, \vect{v} \rangle = 5v_1^2 - 6v_1v_2 + 2v_2^2 = 
\frac{1}{5}[(5v_1 - 3v_2)^2 + v_2^2] $

\setcounter{enumi}{3}
\item  $ \langle \vect{v}, \vect{v} \rangle = 3v_1^2 + 8v_1v_2 + 6v_2^2 = 
\frac{1}{3}[(3v_1 + 4v_2)^2 + 2v_2^2] $

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
In each case, find a symmetric matrix $A$ such that $\langle\vect{v}, \vect{w}\rangle = \vect{v}^{T}A\vect{w}$.

{\footnotesize
\begin{enumerate}[label={\alph*.}]
\item 
$  \left\langle
\leftB \begin{array}{r}
v_1 \\
v_2
\end{array} \rightB, \leftB \begin{array}{r}
w_1 \\
w_2
\end{array} \rightB
\right\rangle
= v_1w_1 + 2v_1w_2 + 2v_2w_1 + 5v_2w_2$

\item 
$\left\langle
\leftB \begin{array}{r}
v_1 \\
v_2
\end{array} \rightB, \leftB \begin{array}{r}
w_1 \\
w_2
\end{array} \rightB
\right\rangle
= v_1w_1 - v_1w_2 - v_2w_1 + 2v_2w_2$

\item 
$\left\langle
\leftB \begin{array}{r}
v_1 \\
v_2 \\
v_3
\end{array} \rightB, \leftB \begin{array}{r}
w_1 \\
w_2 \\
w_3
\end{array} \rightB
\right\rangle
= 2v_1w_1 + v_2w_2 + v_3w_3 - v_1w_2 \\ \hspace*{10em}-v_2w_1 + v_2w_3 + v_3w_2$

\item 
$\left\langle
\leftB \begin{array}{r}
v_1 \\
v_2 \\
v_3
\end{array} \rightB, \leftB \begin{array}{r}
w_1 \\
w_2 \\
w_3
\end{array} \rightB
\right\rangle
= v_1w_1 + 2v_2w_2 + 5v_3w_3 \\ \hspace*{10em}- 2v_1w_3 - 2v_3w_1$

\end{enumerate}}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item $\leftB \begin{array}{rr}
1 & -2 \\
-2 & 1
\end{array} \rightB$

\setcounter{enumi}{3}
\item $\leftB \begin{array}{rrr}
1 & 0 & -2 \\
0 & 2 & 0 \\
-2 & 0 & 5
\end{array} \rightB$

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
If $A$ is symmetric and $\vect{x}^{T}A\vect{x} = 0$ for all columns $\vect{x}$ in $\RR^n$, show that $A = 0$. [\textit{Hint}: Consider
$\langle \vect{x} + \vect{y}, \vect{x} + \vect{y} \rangle$ where $\langle \vect{x}, \vect{y} \rangle = \vect{x}^TA\vect{y}$.]

\begin{sol}
By the condition, $\langle \vect{x}, \vect{y} \rangle = \frac{1}{2} \langle \vect{x} + \vect{y}, \vect{x} + \vect{y} \rangle = 0$ for all $\vect{x}$, $\vect{y}$. Let $\vect{e}_{i}$ denote column $i$ of $I$. If $A = \leftB a_{ij} \rightB$, then $a_{ij} = \vect{e}_{i}^{T}A\vect{e}_{j} = \{\vect{e}_{i}, \vect{e}_{j}\} = 0$ for all $i$ and $j$.
\end{sol}
\end{ex}

\begin{ex}
Show that the sum of two inner products on $V$ is again an inner product.
\end{ex}

\begin{ex} \label{ex:10_1_16}
Let $ \vectlength \vect{u} \vectlength = 1$, $\vectlength \vect{v} \vectlength = 2$, $\vectlength \vect{w} \vectlength = \sqrt{3} $, $\langle \vect{u}, \vect{v} \rangle = -1$, $\langle\vect{u}, \vect{w}\rangle = 0$ and $\langle\vect{v}, \vect{w}\rangle = 3$. Compute:
\begin{exenumerate}
\exitem $\langle \vect{v} + \vect{w}, 2\vect{u} - \vect{v} \rangle$
\exitem $\langle \vect{u} - 2 \vect{v} - \vect{w}, 3\vect{w} - \vect{v} \rangle$
\end{exenumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  $-15$

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
Given the data in Exercise \ref{ex:10_1_16}, show that $\vect{u} + \vect{v} = \vect{w}$.
\end{ex}

\begin{ex}
Show that no vectors exist such that $\vectlength\vect{u}\vectlength = 1$, $\vectlength\vect{v}\vectlength = 2$, and $\langle\vect{u}, \vect{v}\rangle = -3$.
\end{ex}

\begin{ex} \label{ex:10_1_19}
Complete Example~\ref{exa:030310}.
\end{ex}

\begin{ex} \label{ex:10_1_20}
Prove Theorem~\ref{thm:030346}.

\begin{sol}
\textbf{1.} Using P2: 
$ \langle \vect{u}, \vect{v} + \vect{w} \rangle = 
\langle \vect{v} + \vect{w}, \vect{u} \rangle = 
\langle \vect{v}, \vect{u} \rangle + \langle \vect{w}, \vect{u} \rangle = 
\langle \vect{u}, \vect{v} \rangle + \langle \vect{u}, \vect{w} \rangle$.

\textbf{2.} Using P2 and P4: 
$\langle \vect{v}, r\vect{w} \rangle =
\langle r\vect{w}, \vect{v} \rangle =
r \langle \vect{w}, \vect{v} \rangle =
r \langle \vect{v}, \vect{w} \rangle$.

\textbf{3.} Using P3: 
$\langle \vect{0}, \vect{v} \rangle =
\langle \vect{0} + \vect{0}, \vect{v} \rangle =
\langle \vect{0}, \vect{v} \rangle + \langle \vect{0}, \vect{v} \rangle
$, so $ \langle \vect{0}, \vect{v} \rangle = 0$. The rest is P2.

\textbf{4.} Assume that $\langle \vect{v}, \vect{v} \rangle = 0$. If $\vect{v} \neq \vect{0}$ this contradicts P5, so $\vect{v} = \vect{0}$. Conversely, if $\vect{v} = \vect{0}$, then $\langle \vect{v}, \vect{v} \rangle = 0$ by Part 3 of this theorem.
\end{sol}
\end{ex}

\begin{ex} \label{ex:10_1_21}
Prove Theorem~\ref{thm:030545}.
\end{ex}

\begin{ex}
Let $\vect{u}$ and $\vect{v}$ be vectors in an inner product space $V$.

\begin{enumerate}[label={\alph*.}]
\item Expand $\langle2\vect{u} - 7\vect{v}, 3\vect{u} + 5\vect{v} \rangle$.

\item Expand $\langle3\vect{u} - 4\vect{v}, 5\vect{u} + \vect{v} \rangle$.

\item Show that $\vectlength \vect{u} + \vect{v} \vectlength ^2 = \vectlength \vect{u} \vectlength ^2 + 2 \langle \vect{u}, \vect{v} \rangle + \vectlength \vect{v} \vectlength ^2 $.

\item Show that $\vectlength \vect{u} - \vect{v} \vectlength ^2 = \vectlength \vect{u} \vectlength ^2 - 2 \langle \vect{u}, \vect{v} \rangle + 
\vectlength \vect{v} \vectlength ^2$.

\end{enumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  $15\vectlength\vect{u}\vectlength^{2} - 17 \langle \vect{u}, \vect{v} \rangle - 4\vectlength\vect{v}\vectlength^{2}$

\setcounter{enumi}{3}
\item  $\vectlength\vect{u} + \vect{v}\vectlength^{2} = \langle \vect{u} + \vect{v}, \vect{u} + \vect{v} \rangle = \vectlength\vect{u}\vectlength^{2} + 2\langle \vect{u}, \vect{v}\rangle + \vectlength\vect{v}\vectlength^{2}$

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
Show that 
\begin{equation*}
\vectlength \vect{v} \vectlength ^2 +
\vectlength \vect{w} \vectlength ^2 = \frac{1}{2} \{
\vectlength \vect{v} + \vect{w} \vectlength ^2 +
\vectlength \vect{v} - \vect{w} \vectlength ^2\}
\end{equation*}
for any $\vect{v}$ and $\vect{w}$ in an inner product space.
\end{ex}

\begin{ex}
Let $\langle\ , \rangle$ be an inner product on a vector space $V$. Show that the corresponding distance function is translation invariant. That is, show that \newline $\func{d}(\vect{v}, \vect{w}) = \func{d}(\vect{v} + \vect{u}, \vect{w} + \vect{u})$ for all $\vect{v}$, $\vect{w}$, and $\vect{u}$ in $V$.
\end{ex}

\begin{ex}
\begin{enumerate}[label={\alph*.}]
\item Show that $\langle \vect{u}, \vect{v} \rangle = \frac{1}{4}[\vectlength \vect{u} + \vect{v} \vectlength ^2 - \vectlength \vect{u} - \vect{v} \vectlength ^2]$ for all $\vect{u}$, $\vect{v}$ in an inner product space $V$.

\item If $\langle\ , \rangle$ and $\langle\ , \rangle^\prime$ are two inner products on $V$ that have equal associated norm functions, show that $\langle\vect{u}, \vect{v}\rangle = \langle\vect{u}, \vect{v}\rangle^\prime$ holds for all $\vect{u}$ and $\vect{v}$.

\end{enumerate}
\end{ex}

\begin{ex}
Let $\vect{v}$ denote a vector in an inner product space $V$.

\begin{enumerate}[label={\alph*.}]
\item Show that $W = \{\vect{w} \mid \vect{w} \mbox{ in } V, \langle\vect{v}, \vect{w} = 0\}$ is a subspace of $V$.

\item Let $W$ be as in (a). If $V = \RR^3$ with the dot product, and if $\vect{v} = (1, -1, 2)$, find a basis for $W$.

\end{enumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  $\{(1, 1, 0), (0, 2, 1)\}$

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex} \label{ex:10_1_27}
Given vectors $\vect{w}_{1}, \vect{w}_{2}, \dots, \vect{w}_{n}$ and $\vect{v}$, assume that $\langle\vect{v}, \vect{w}_{i}\rangle = 0$ for each $i$. Show that $\langle\vect{v}, \vect{w}\rangle = 0$ for all $\vect{w}$ in $\func{span}\{\vect{w}_{1}, \vect{w}_{2}, \dots, \vect{w}_{n}\}$.
\end{ex}

\begin{ex}
If $V = \func{span}\{\vect{v}_{1}, \vect{v}_{2}, \dots, \vect{v}_{n}\}$ and $\langle\vect{v}, \vect{v}_{i}\rangle = \langle\vect{w}, \vect{v}_i\rangle$ holds for each $i$. Show that $\vect{v} = \vect{w}$.

\begin{sol}
$\langle \vect{v} - \vect{w}, \vect{v}_{i} \rangle = \langle \vect{v}, \vect{v}_{i} \rangle - \langle \vect{w}, \vect{v}_{i} \rangle = 0$ for each $i$, so $\vect{v} = \vect{w}$ by Exercise \ref{ex:10_1_27}.
\end{sol}
\end{ex}

\begin{ex}
Use the Cauchy-Schwarz inequality in an inner product space to show that:

\begin{enumerate}[label={\alph*.}]
\item If $\vectlength\vect{u}\vectlength \leq 1$, then $\langle\vect{u}, \vect{v}\rangle^{2} \leq \vectlength\vect{v}\vectlength^{2}$ for all $\vect{v}$ in $V$.

\item $(x \cos \theta + y \sin \theta)^{2} \leq x^{2} + y^{2}$ for all real $x$, $y$, and $\theta$.

\item $\vectlength r_1\vect{v}_1 + \dots + r_n\vect{v}_n \vectlength ^2 \leq [r_1 \vectlength \vect{v}_1 \vectlength + \dots + r_n \vectlength \vect{v}_n \vectlength ]^2$
for all vectors $\vect{v}_{i}$, and all $r_{i} > 0$ in $\RR$.

\end{enumerate}
\begin{sol}
\begin{enumerate}[label={\alph*.}]
\setcounter{enumi}{1}
\item  If $\vect{u} = (\cos \theta, \sin \theta)$ in $\RR^2$ (with the dot product) then $\vectlength\vect{u}\vectlength = 1$. Use \textbf{(a)} with $\vect{v} = (x, y)$.

\end{enumerate}
\end{sol}
\end{ex}

\begin{ex}
If $A$ is a $2 \times n$ matrix, let $\vect{u}$ and $\vect{v}$ denote the rows of $A$.

\begin{enumerate}[label={\alph*.}]
\item Show that 
$AA^T = \leftB \begin{array}{rr}
\vectlength \vect{u} \vectlength ^2 & \vect{u} \dotprod \vect{v} \\
\vect{u} \dotprod \vect{v} & \vectlength \vect{v} \vectlength ^2
\end{array} \rightB$.

\item Show that $\func{det}(AA^{T}) \geq 0$.

\end{enumerate}
\end{ex}

\begin{ex} \label{ex:10_1_31}
\begin{enumerate}[label={\alph*.}]
\item If $\vect{v}$ and $\vect{w}$ are nonzero vectors in an inner product space $V$, show that 
$-1 \leq \frac{\langle \vect{v}, \vect{w} \rangle}{\vectlength \vect{v} \vectlength \vectlength \vect{w} \vectlength} \leq 1$, and hence that a unique angle $\theta$ exists such that \newline $\frac{\langle \vect{v}, \vect{w} \rangle}{\vectlength \vect{v} \vectlength \vectlength \vect{w} \vectlength} = \cos \theta$ and $0 \leq \theta \leq \pi$. This angle $\theta$ is called the
\textbf{angle between} $\vect{v}$ and $\vect{w}$\index{angles!angle between two vectors}.

\item Find the angle between $\vect{v} = (1, 2, -1, 1\, 3)$ and $\vect{w} = (2, 1, 0, 2, 0)$ in $\RR^5$ with the dot product.

\item If $\theta$ is the angle between $\vect{v}$ and $\vect{w}$, show that the \textbf{law of cosines}\index{law of cosines} is valid:
\begin{equation*}
\vectlength \vect{v} - \vect{w} \vectlength = \vectlength \vect{v} \vectlength ^2 + \vectlength \vect{w} \vectlength ^2 - 2\vectlength \vect{v} \vectlength \vectlength \vect{w} \vectlength \cos \theta.
\end{equation*}
\end{enumerate}
\end{ex}

\begin{ex}
If $V = \RR^2$, define $\vectlength(x, y)\vectlength = |x| + |y|$.

\begin{enumerate}[label={\alph*.}]
\item Show that $\vectlength\dotprod\vectlength$ satisfies the conditions in Theorem~\ref{thm:030504}.

\item Show that $\vectlength\dotprod\vectlength$ does not arise from an inner product on $\RR^2$ given by a matrix $A$. [\textit{Hint}: If it did, use Theorem~\ref{thm:030372} to find numbers $a$, $b$, and $c$ such that $\vectlength(x, y)\vectlength^{2} = ax^{2} + bxy + cy^{2}$ for all $x$ and $y$.]

\end{enumerate}
\end{ex}
\end{multicols}
